{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic terminologies and questions!\n",
    "\n",
    "\"Curiousity is the key to find answers to the most terrible problems\"\n",
    "\n",
    "\n",
    "\n",
    "However, in this example our example is not that terrible! The power of neural networks has been proved time and again and state of the art innovations in pattern recognition, computer vision, speech recognition etc. have paved ways for better understanding into how a machine actually thinks! Days are still far away when computers would think like we do but the good better part is, the quest has begun! With the emergence of high computational power and improvements in distributed computing that equips us to play with even bigger data with less compute has been tremendous. 2017 it is and probably this is the perfect time to get your hands dirty in the really 'deep' world of neural networks!\n",
    "\n",
    "To start with tensorflow, instead of going for a MNIST or an IRIS dataset, let's try to understand how the neural nets work , firstly, without tensorflow or any libraries and then with the most popular framework that rules, Tensorflow!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 2000 Cost: 6.79488e-06\n",
      "Epoch 400 completed out of 2000 Cost: 0.000229276\n",
      "Epoch 800 completed out of 2000 Cost: 0.000113623\n",
      "Epoch 1200 completed out of 2000 Cost: 7.46492e-05\n",
      "Epoch 1600 completed out of 2000 Cost: 5.51909e-05\n",
      "Accuracy: 0.75\n",
      "prediction for : [0, 0, 0, 1]\n",
      "0.000300691 0.999658\n",
      "prediction for : [1, 1, 0, 1]\n",
      "0.177402 0.874036\n",
      "prediction for : [0, 1, 0, 1]\n",
      "0.000200887 0.998954\n",
      "prediction for : [0, 0, 1, 0]\n",
      "0.000116789 0.999905\n"
     ]
    }
   ],
   "source": [
    "# Intro to tensorflow using toy data set\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "def feature_set(test_size):\n",
    "    #print(\"All imported perfectly!\")\n",
    "\n",
    "    # after importing the necessary dependencies or packages, we are good to go with our toy dataset!\n",
    "\n",
    "    # This would be quite intuitive than the 4-bit comparator and hence I guess we could train a model \n",
    "    #that predicts stuff really well\n",
    "\n",
    "    features = []\n",
    "    features.append([[0,0,0,0],[0,1]])\n",
    "    features.append([[0,0,0,1],[0,1]])\n",
    "    features.append([[0,0,1,0],[0,1]])\n",
    "    features.append([[0,0,1,1],[0,1]])\n",
    "    features.append([[0,1,0,0],[0,1]])\n",
    "    features.append([[0,1,0,1],[0,1]])\n",
    "    features.append([[0,1,1,0],[0,1]])\n",
    "    features.append([[0,1,1,1],[0,1]])\n",
    "    features.append([[1,0,0,0],[0,1]])\n",
    "    features.append([[1,0,0,1],[0,1]])\n",
    "    features.append([[1,0,1,0],[0,1]])\n",
    "    features.append([[1,0,1,1],[1,0]])\n",
    "    features.append([[1,1,0,0],[0,1]])\n",
    "    features.append([[1,1,0,1],[1,0]])\n",
    "    features.append([[1,1,1,0],[0,1]])\n",
    "    features.append([[1,1,1,1],[1,0]])\n",
    "\n",
    "    #you can see the output is dependent upon the first and last bits in the input, when both of them are 1 its 1 else the \n",
    "    #answer is 0, lets see whether tensorflow is powerful enough to crack this!\n",
    "\n",
    "    #print(features)\n",
    "\n",
    "    #now lets convert this to the np array but lets shuffle it first\n",
    "\n",
    "    random.shuffle(features)\n",
    "    features = np.array(features)\n",
    "\n",
    "    #print(features)\n",
    "    \n",
    "    #fixing testing size, i.e., how much data do you split for testing and training purposes!\n",
    "    testing_size = int(len(features)*test_size)\n",
    "    \n",
    "    train_x = list(features[:,0][:-testing_size])\n",
    "    test_x = list(features[:,0][-testing_size:])\n",
    "    \n",
    "    train_y = list(features[:,1][:-testing_size])\n",
    "    test_y=list(features[:,1][-testing_size:])\n",
    "    \n",
    "    #print(len(train_x))\n",
    "    #print(len(test_x))\n",
    "    \n",
    "    return train_x,test_x,train_y,test_y\n",
    "    \n",
    "    \n",
    "\n",
    "#feature_set(0.3)\n",
    "#feature_set(0.4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#as our feature set is ready lets begin with our tensorflow model\n",
    "\n",
    "#asign the returned value to respective variables of train and test\n",
    "\n",
    "train_x, test_x, train_y, test_y = feature_set(0.3)\n",
    "\n",
    "#let define the no. of nodes of our hidden layers:\n",
    "\n",
    "n_nodes_h11 = 40\n",
    "n_nodes_h12 = 40\n",
    "\n",
    "#classes in our output\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "#no. of iterations and batch size:\n",
    "\n",
    "hm_epochs = 2000\n",
    "batch_size = 4\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "# random weights and biases\n",
    "\n",
    "hidden_1_layer = {'f_fum':n_nodes_h11,\n",
    "                 'weight': tf.Variable(tf.random_normal([len(train_x[0]),n_nodes_h11])),\n",
    "                 'bias': tf.Variable(tf.random_normal([n_nodes_h11]))}\n",
    "\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_h12,\n",
    "                 'weight': tf.Variable(tf.random_normal([n_nodes_h11,n_nodes_h12])),\n",
    "                 'bias': tf.Variable(tf.random_normal([n_nodes_h12]))}\n",
    "\n",
    "\n",
    "output_layer = {'f_fum':None,\n",
    "                 'weight': tf.Variable(tf.random_normal([n_nodes_h12,n_classes])),\n",
    "                 'bias': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    #hidden layer general :  (data * W) + bias\n",
    "    \n",
    "    #hidden layer 1\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']),hidden_1_layer['bias'])\n",
    "    l1 = tf.sigmoid(l1)\n",
    "    \n",
    "    # hidden layer 2, now data is the output of the first hidden layer\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.sigmoid(l2)\n",
    "    \n",
    "    # output layer, now i/p is l2\n",
    "    \n",
    "    output = tf.add(tf.matmul(l2,output_layer['weight']),output_layer['bias'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "#traing our model\n",
    "\n",
    "def train_neural(x):\n",
    "    \n",
    "    #use model definition\n",
    "    \n",
    "    prediction = neural_network_model(x)\n",
    "    \n",
    "    #cost fucntion\n",
    "    \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "\n",
    "    # optimize for cost using gradient descent, here's the key thing!\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1).minimize(cost)\n",
    "    \n",
    "    #TF session\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #initializing variables\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #loop through specified no. of iterations\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            i=0\n",
    "            \n",
    "            #handle batch sized chunks of training data\n",
    "            \n",
    "            while i<len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                \n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                \n",
    "                # the black box begins\n",
    "                \n",
    "                _, c =sess.run([optimizer,cost], feed_dict={x: batch_x,y:batch_y})\n",
    "                \n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "                last_cost = c\n",
    "                \n",
    "            if(epoch%(hm_epochs/5))==0:\n",
    "                print('Epoch',epoch, 'completed out of',hm_epochs,'Cost:', last_cost)\n",
    "                \n",
    "        \n",
    "        #print accuracy\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print('Accuracy:', accuracy.eval({x:test_x, y:test_y}))\n",
    "        \n",
    "        #print predictions\n",
    "        \n",
    "        for i,t in enumerate(test_x):\n",
    "            print('prediction for :', test_x[i])\n",
    "            output = prediction.eval(feed_dict = {x: [test_x[i]]})\n",
    "            # normalise the predictui values:\n",
    "            print(tf.sigmoid(output[0][0]).eval(), tf.sigmoid(output[0][1]).eval())\n",
    "            \n",
    "train_neural(x)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
